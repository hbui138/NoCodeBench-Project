# NoCode-bench-ATSE-TUM
An AI agent that generates code patches from documentation changes. Project for "Advanced Topics in Software Engineering" at TUM.

# NoCode-Agent Setup Guide (Steps 1–2)

This guide summarizes the essential steps required to set up the project environment for **NoCode-Agent**, including project structure, environment setup, dependencies, Docker configuration, and dataset preparation.

---

## ✅ Step 1 — Initialize Project & Environment

### **1.1 Clone NoCode-bench Core**

Clone the official benchmark repository into a dedicated folder so your own backend/frontend remain clean:

```bash
git clone https://github.com/hbui138/NoCode-bench.git bench-core
```

Folder layout after this step:

```
nocode-agent/
├── backend/
├── bench_core/   # NoCode-bench source code
├── frontend/
└── .git/
```

### **1.2 Create Conda Environment (Python 3.12)**

The benchmark requires Python **3.12**, so create and activate the environment:

```bash
conda create -n ncb python=3.12 -y
conda activate ncb
```

> **Important:** Always activate this environment before running backend or benchmark scripts.

### **1.3 Install Required Libraries**

#### A) **Install dependencies for NoCode-bench core**

```bash
cd bench_core
pip install -r requirements.txt
cd ..
```

#### B) **Install backend dependencies**

Create `backend/requirements.txt` containing:

```
fastapi
uvicorn
python-multipart
datasets
docker
openai
requests
```

Then install:

```bash
pip install -r backend/requirements.txt
```

---

## ✅ Step 2 — Prepare Docker & Datasets

### **2.1 Install Docker Desktop**

Make sure Docker Desktop is installed and running:

* Windows/macOS: Install Docker Desktop
* Linux: Install Docker Engine + Docker Compose plugin

Verify installation:

```bash
docker --version
docker compose version
```

### **2.2 Pull Required Docker Images**

Move into the environment directory inside the benchmark core:

```bash
cd bench_core/environment
```

Pull repository-level images using the provided script:

```bash
bash ./pull_from_hub.sh
```

This downloads all base Docker images required for running the benchmark tasks.

### **2.3 Verify Dataset Availability**

Use the built‑in script to check whether datasets are correctly downloaded:

```bash
python check_data.py
```

If any dataset is missing, the script will notify you and provide hints for fixing the issue.

The data has following columns

```bash
repo: (str) - The repository owner/name identifier from GitHub.
instance_id: (str) - A formatted instance identifier, usually as repo_owner__repo_name-PR-number.
html_url: (str) - The URL of the PR web page where the instances are collected. 
feature_patch: (str) - The gold patch, the patch generated by the PR (minus test-related code), that resolved the issue.
test_patch: (str) - A test-file patch that was contributed by the solution PR.
doc_changes: (list) - The documentation changes in a certain PR.
version: (str) - Installation version to use for running evaluation.
base_commit: (str) - The commit hash of the repository representing the HEAD of the repository before the solution PR is applied.
PASS2PASS: (str) - A json list of strings that represent tests that should pass before and after the PR application.
FAIL2PASS: (str) - A json list of strings that represent the set of tests resolved by the PR and tied to the issue resolution.
augmentations: (dict) - A set of names of the entity used to implement the feature.
mask_doc_diff: (list) - The documentation changes in a certain PR, which masked the PR number.
problem_statement: (str) - The main input contained `mask_doc_diff` and `augmentations`.
```

Run the pipeline

Run the backend server
```bash
python ./backend/main.py
```

Run the test (one instance or all instances)
If one, change the desired task number inside test_one.py and run (Line 27:         target_task = tasks[0])
```bash
python test_one.py
python test_all.py
```

Folder structure full
```bash
nocode-agent
  .github/workflows/
  backend
    agent.py
    main.py
    requirements.txt
    schemas.py
    state.py
    utils.py
    service.py
  bench-core (folder you clone the nocodebench repo)
    requirements.txt (modules used by bench-core)
    repos
    evaluation
  frondend
  results (What you get after running the tests)
    results_timestamp
      evaluation_logs
      all_preds.jsonl
      summary_report.txt
  .env (You created)
  test_all.py
  test_one.py
```


